{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from large dataset using the chunksize parameter\n",
    "\n",
    "This mini-tut shows how to read data from a very large dataset (I've also added a line to split it into multiple CSV files (so that it's easier to read over Excel\n",
    "\n",
    "Recall that the chunksize returns an iterable, so once a portion is read, it is \"junked\"\n",
    "\n",
    "This sample file has 7million rows and 10 features, and so far, Excel, Power Tools (Power Query etc) have struggled... So here comes Python to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reader = pd.read_csv('large_file.csv', sep='|', chunksize=100000)\n",
    "for i, chunk in enumerate(reader):\n",
    "    chunk.to_csv(str(i)+'.csv')\n",
    "    print(f\"done{i}\") # to track each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm the saved data, we can use a combination of pandas and glob\n",
    "import glob\n",
    "for i, filename in enumerate(glob.glob('*.csv')):\n",
    "    print(i, pd.read_csv(filename).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
