{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions\n",
    "<br>\\w mathes any letter or digit, e.g. 'a'\n",
    "<br>\\d matches digit, e.g. 9\n",
    "<br>\\s matches space , e.g. ' '\n",
    "<br>.* is a wildcard character to match any word or symbol\n",
    "<br> + or * is for greedy match, + is 1 or more, * is zero or more i.e. \\w+ matches any word, e.g. 'Magic'\n",
    "<br>\\S is for NOT space (caps is for NOT)\n",
    "<br>[a-z] is for all a to z inclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Import the re module to use regular expressions, i.e. import re\n",
    "<br>To split a string on regex, use split. Returns a list of all strings EXCEPT the matched strings\n",
    "<br>To find all patters in a string, use findall\n",
    "<br>To search for a pattern, use search\n",
    "<br>To match a pattern, use re.match(), e.g. re.match('abc', 'abcdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RegEx!', 'fun?', 'so.', 'sentences?', 'words?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Match all words that end in a symbol, i.e. !, ?, . which translates to either \\w+\\. \\w+\\? or \\w+\\!\n",
    "re.findall('\\w+\\.|\\w+\\?|\\w+\\!', mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's write \",\n",
       " \"  Won't that be \",\n",
       " '  I sure think ',\n",
       " '  Can you find 4 ',\n",
       " '  Or perhaps, all 19 ',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\w+\\.|\\w+\\?|\\w+\\!', mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', 'RegEx', 'Won', 'Can', 'Or']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all capitalised words\n",
    "re.findall(\"[A-Z]\\w+\",mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many SINGLE spaces are there?\n",
    "len(re.findall(\"\\s\",mystring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many complete spaces are there? i.e. count two spaces as one\n",
    "len(re.findall(\"\\s+\",mystring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many digits\n",
    "len(re.findall(\"\\d+\", mystring))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Hi there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', \"n't\", 'like', 'Sam', \"'s\", 'shoes']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I don't like Sam's shoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>sent_tokenize tokenizes a document into SENTENCES\n",
    "<br>regexp_tokenize tokenizes a string or document based on a regular expression pattern\n",
    "<br>TweetTokenizer is for tweets. Allows for separation of hashtags, mentions and other symbol groups like \n",
    "exclamations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(2, 4), match='cd'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Difference between re.search and re.match\n",
    "# re.search searches through the string to match any pattern IN the string\n",
    "# re.match searches FROM THE BEGINNING of the string\n",
    "# They both return iterator objects\n",
    "# See below\n",
    "\n",
    "print(re.search('cd','abcde'))\n",
    "print(re.match('cd','abcde'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the SCENE 1 from the file grail.txt\n",
    "# SCENE 1 is a list of all words from the beginning to the end of SCENE 1\n",
    "# SCENE 2 is the start of the next scene, so we use re.split to separate the scenes into lists and pick the first part [0]\n",
    "# This is rather tacky, but this should do\n",
    "\n",
    "scene_one = re.split(\"SCENE 2\",requests.get(\"https://github.com/kcemenike/GenericPythonCodes/raw/master/data/grail.txt\").text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!',\n",
       " '[clop clop clop] \\nSOLDIER #1: Halt!',\n",
       " 'Who goes there?',\n",
       " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.',\n",
       " 'King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'SOLDIER #1: Pull the other one!',\n",
       " 'ARTHUR: I am, ...  and this is my trusty servant Patsy.',\n",
       " 'We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.',\n",
       " 'I must speak with your lord and master.',\n",
       " 'SOLDIER #1: What?',\n",
       " 'Ridden on a horse?',\n",
       " 'ARTHUR: Yes!',\n",
       " \"SOLDIER #1: You're using coconuts!\",\n",
       " 'ARTHUR: What?',\n",
       " \"SOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\",\n",
       " 'ARTHUR: So?',\n",
       " \"We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\",\n",
       " 'ARTHUR: We found them.',\n",
       " 'SOLDIER #1: Found them?',\n",
       " 'In Mercea?',\n",
       " \"The coconut's tropical!\",\n",
       " 'ARTHUR: What do you mean?',\n",
       " 'SOLDIER #1: Well, this is a temperate zone.',\n",
       " 'ARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?',\n",
       " 'SOLDIER #1: Are you suggesting coconuts migrate?',\n",
       " 'ARTHUR: Not at all.',\n",
       " 'They could be carried.',\n",
       " 'SOLDIER #1: What?',\n",
       " 'A swallow carrying a coconut?',\n",
       " 'ARTHUR: It could grip it by the husk!',\n",
       " \"SOLDIER #1: It's not a question of where he grips it!\",\n",
       " \"It's a simple question of weight ratios!\",\n",
       " 'A five ounce bird could not carry a one pound coconut.',\n",
       " \"ARTHUR: Well, it doesn't matter.\",\n",
       " 'Will you go and tell your master that Arthur from the Court of Camelot is here.',\n",
       " 'SOLDIER #1: Listen.',\n",
       " 'In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?',\n",
       " 'ARTHUR: Please!',\n",
       " 'SOLDIER #1: Am I right?',\n",
       " \"ARTHUR: I'm not interested!\",\n",
       " 'SOLDIER #2: It could be carried by an African swallow!',\n",
       " 'SOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.',\n",
       " \"That's my point.\",\n",
       " 'SOLDIER #2: Oh, yeah, I agree with that.',\n",
       " 'ARTHUR: Will you ask your master if he wants to join my court at Camelot?!',\n",
       " 'SOLDIER #1: But then of course a-- African swallows are non-migratory.',\n",
       " 'SOLDIER #2: Oh, yeah...',\n",
       " \"SOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!\",\n",
       " 'Supposing two swallows carried it together?',\n",
       " \"SOLDIER #1: No, they'd have to have it on a line.\",\n",
       " 'SOLDIER #2: Well, simple!',\n",
       " \"They'd just use a strand of creeper!\",\n",
       " 'SOLDIER #1: What, held under the dorsal guiding feathers?',\n",
       " 'SOLDIER #2: Well, why not?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# Show sentence tokens\n",
    "sent_tokenize(scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',',\n",
       " '.',\n",
       " ':',\n",
       " 'ARTHUR',\n",
       " 'Arthur',\n",
       " 'Camelot',\n",
       " 'I',\n",
       " 'It',\n",
       " 'Pendragon',\n",
       " 'Uther',\n",
       " 'castle',\n",
       " 'from',\n",
       " 'is',\n",
       " 'of',\n",
       " 'son',\n",
       " 'the'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show unique word tokens in the fourth sentence(use the set function)\n",
    "set(word_tokenize(sent_tokenize(scene_one)[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(580, 588), match='coconuts'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"coconuts\",scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[wind] [clop clop clop]', '[clop clop clop]', '[clop clop clop]']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\[.*\\]\",scene_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARTHUR:']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z]\\w+\\:\",sent_tokenize(scene_one)[3]) # Find 'ARTHUR:' in the fourth sentence of scene_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '12', 'dogs']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(('\\d+|\\w+'), 'He has 12 dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABCDefgh', 'abcABC', 'abv']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Za-z]+\", 'ABCDefgh 123 abcABC abv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '45']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[0-9]+\", '1 2 3 45')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.etsl-ng.com', 'http', 'etsl-ng.com']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Za-z\\-\\.]+\", 'www.etsl-ng.com http://etsl-ng.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a-z']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"a-z\", 'a-z a z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['match lowercase phrases including numbers like 12',\n",
       " ' but no symbols',\n",
       " ' like full stops',\n",
       " ' or commas']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[a-z0-9 ]+\", \"match lowercase phrases including numbers like 12, but no symbols, like full stops. or commas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['match',\n",
       " 'lowercase',\n",
       " 'words',\n",
       " 'only',\n",
       " 'including',\n",
       " 'numbers',\n",
       " 'like',\n",
       " '12',\n",
       " 'but',\n",
       " 'no',\n",
       " 'symbols',\n",
       " 'like',\n",
       " 'full',\n",
       " 'stops',\n",
       " 'or',\n",
       " 'commas']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[a-z0-9]+\", \"match lowercase words only including numbers like 12, but no symbols, like full stops. or commas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"(\\w+|#\\d|\\?|!)\", \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#nlp', '#python'], ['#NLP', '#learning'], ['#nlp', '#python']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[regexp_tokenize(tweet, r\"\\#\\w+\") for tweet in tweets] #Tokenize Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#nlp', '#python'], ['#NLP', '#learning'], ['@datacamp', '#nlp', '#python']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[regexp_tokenize(tweet, r\"[\\#|\\@]\\w+\") for tweet in tweets] #hashtags and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  '#nlp',\n",
       "  'exercise',\n",
       "  'ive',\n",
       "  'found',\n",
       "  'online',\n",
       "  '!',\n",
       "  '#python'],\n",
       " ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'],\n",
       " ['Thanks', '@datacamp', ':)', '#nlp', '#python']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[TweetTokenizer().tokenize(tweet) for tweet in tweets] #Using TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode tokenization\n",
    "Unicode ranges for emoji are below:\n",
    "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')\n",
    "\n",
    "Convert unicode to character using chr and vice-versa using ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text = \"Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['🍕', '🚕']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize capital letters, including german unicode capital letters\n",
    "\n",
    "regexp_tokenize(german_text, \"[\\U0001F300-\\U0001F5FF|\\U0001F600-\\U0001F64F|\\U0001F680-\\U0001F6FF|\\u2600-\\u26FF|\\u2700-\\u27BF]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ĭ'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128661"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('🚕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🚕'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(128661)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: How long is each line in the grail.txt (excluding the character names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "grail = requests.get(\"https://github.com/kcemenike/GenericPythonCodes/raw/master/data/grail.txt\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop clop clop] \n",
      "SOLDIER #1: Halt!  Who goes there?\n",
      "ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\n",
      "SOLDIER #1: Pull the other one!\n",
      "ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\n",
      "SOLDIER #1: What?  Ridden on a horse?\n",
      "ARTHUR: Yes!\n",
      "SOLDIER #1: You're using coconuts!\n",
      "ARTHUR: What?\n"
     ]
    }
   ],
   "source": [
    "# Split text into lines\n",
    "lines = grail.split('\\n')\n",
    "for line in lines[:10]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SCENE 1:'],\n",
       " ['KING ARTHUR:'],\n",
       " ['SOLDIER #1:'],\n",
       " ['ARTHUR:'],\n",
       " ['SOLDIER #1:'],\n",
       " ['ARTHUR:'],\n",
       " ['SOLDIER #1:'],\n",
       " ['ARTHUR:'],\n",
       " ['SOLDIER #1:'],\n",
       " ['ARTHUR:']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace prompts such as 'KING ARTHUR:' and 'SOLDIER #1:' etc\n",
    "# It should be from the start of a line to the first ':'\n",
    "# Build and test the regex pattern first\n",
    "[re.findall(\"[A-Z]+\\s*.*#*\\w*:\", line) for line in lines][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[374, 1191]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are there any \"uncaught\" patterns? If uncaught, findall will return a blank list (with length = 0)\n",
    "# We can check for any items with blank list\n",
    "# Enumerate helps us return the index of the \"culprits\"\n",
    "[index for index, item in enumerate([re.findall(\"[A-Z]+\\s*.*#*\\w*:\", line) for line in lines]) if len(item)==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His head smashed in and his heart cut out, And his liver removed and his bowels unplugged, And his nostrils raped and his bottom burned off, And his pen--']\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "for line in [index for index, item in enumerate([re.findall(\"[A-Z]+\\s*.*#*\\w*:\", line) for line in lines]) if len(item)==0]:\n",
    "    print ([lines[line]])\n",
    "# You will see that index 374 is a false positive, and 1191 is the end of the full text (hence the presence of a blank line), so we're good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] ',\n",
       " 'KING ARTHUR: Whoa there!  [clop clop clop] ',\n",
       " 'SOLDIER #1: Halt!  Who goes there?',\n",
       " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'SOLDIER #1: Pull the other one!']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's replace using re.sub\n",
    "# Let's review what lines was before replacement\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' [wind] [clop clop clop] ',\n",
       " ' Whoa there!  [clop clop clop] ',\n",
       " ' Halt!  Who goes there?',\n",
       " ' It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " ' Pull the other one!']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And after replacement...\n",
    "lines = [re.sub(\"[A-Z]+\\s*.*#*\\w*:\", '', line) for line in lines]\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize each line in lines\n",
    "Should we use word_tokenize or regexp_tokenize with the \"\\w+\" pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wind', 'clop', 'clop', 'clop'],\n",
       " ['Whoa', 'there', 'clop', 'clop', 'clop'],\n",
       " ['Halt', 'Who', 'goes', 'there']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word Tokenize\n",
    "[regexp_tokenize(line, \"\\w+\") for line in lines][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'wind', ']', '[', 'clop', 'clop', 'clop', ']'],\n",
       " ['Whoa', 'there', '!', '[', 'clop', 'clop', 'clop', ']'],\n",
       " ['Halt', '!', 'Who', 'goes', 'there', '?']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regexp_tokenize\n",
    "[word_tokenize(line) for line in lines][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use regexp_tokenize (word_tokenize looks too noisy)\n",
    "tokenized_lines = [regexp_tokenize(line, \"\\w+\") for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the length of each line as a histogram\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist([len(line) for line in tokenized_lines])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
